{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7L1H_fx0DI0"
   },
   "source": [
    "# Project Title: Mini RAG\n",
    "### A Lightweight, Colab-Friendly RAG Demo using ChromaDB + Hugging Face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWrXGBtPjJut"
   },
   "source": [
    "#RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paRoUg7KiqUC",
    "outputId": "4035076a-025d-49e4-976a-66e33b6c1f89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Acceptance\n",
      "Citations: [{'label': '[1]', 'source': 'jd4', 'section': 'Acceptance', 'snippet': 'acceptance : working url ; query → retrieved chunks → reranked → llm answer with citations visible..'}, {'label': '[2]', 'source': 'jd2', 'section': 'LLM', 'snippet': 'use any provider... generate grounded answers with citations.'}, {'label': '[3]', 'source': 'jd', 'section': 'Track B', 'snippet': 'goal : build and host a small rag app...'}, {'label': '[4]', 'source': 'jd3', 'section': 'Frontend', 'snippet': 'frontend : upload / paste area, query box, answers panel... hosting : deploy on free host...'}]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Mini RAG (ChromaDB + HF)\n",
    "# Single-block, Colab-friendly, production-minded\n",
    "# =========================\n",
    "\n",
    "# --- Install dependencies ---\n",
    "import sys, subprocess, importlib.util\n",
    "\n",
    "def pip_install(pkgs):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + pkgs)\n",
    "\n",
    "required = [\n",
    "    \"chromadb>=0.5.3\",\n",
    "    \"sentence-transformers>=2.6.1\",\n",
    "    \"transformers>=4.44.0\",\n",
    "    \"torch>=2.1.0\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"tqdm\",\n",
    "    \"rank_bm25\",\n",
    "    \"python-dotenv\"\n",
    "]\n",
    "\n",
    "# ✅ Use importlib.util.find_spec instead of pkgutil.find_loader\n",
    "missing = [\n",
    "    p for p in required\n",
    "    if importlib.util.find_spec(p.split(\"==\")[0].split(\">=\")[0]) is None\n",
    "]\n",
    "\n",
    "if missing:\n",
    "    pip_install(missing)\n",
    "\n",
    "\n",
    "# --- Imports ---\n",
    "import os, uuid, json, time, logging\n",
    "import numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# --- Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "logger = logging.getLogger(\"mini_rag\")\n",
    "\n",
    "# --- Config ---\n",
    "load_dotenv()\n",
    "CONFIG = {\n",
    "    \"PERSIST_DIR\": \"/content/chroma_store\",\n",
    "    \"COLLECTION_NAME\": \"mini_rag_docs\",\n",
    "    \"EMBED_MODEL\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"RERANK_MODEL\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    \"GEN_MODEL\": \"t5-base\",\n",
    "    \"CHUNK_TOKENS_MIN\": 800,\n",
    "    \"CHUNK_TOKENS_MAX\": 1200,\n",
    "    \"CHUNK_OVERLAP_RATIO\": 0.12,\n",
    "    \"TOP_K\": 8,\n",
    "    \"FINAL_K\": 4,\n",
    "    \"MMR_LAMBDA\": 0.5,\n",
    "    \"MAX_NEW_TOKENS\": 256,\n",
    "    \"TEMPERATURE\": 0.3\n",
    "}\n",
    "\n",
    "# --- Tokenizer for chunking ---\n",
    "TOKENIZER_FOR_CHUNK = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def count_tokens(text): return len(TOKENIZER_FOR_CHUNK.encode(text, add_special_tokens=False))\n",
    "def chunk_text(text, min_tokens=CONFIG[\"CHUNK_TOKENS_MIN\"], max_tokens=CONFIG[\"CHUNK_TOKENS_MAX\"], overlap_ratio=CONFIG[\"CHUNK_OVERLAP_RATIO\"]):\n",
    "    tokens = TOKENIZER_FOR_CHUNK.encode(text, add_special_tokens=False)\n",
    "    chunks, i, overlap = [], 0, int(max_tokens * overlap_ratio)\n",
    "    while i < len(tokens):\n",
    "        end = min(i + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[i:end]\n",
    "        if len(chunk_tokens) < min_tokens and end < len(tokens):\n",
    "            end = min(i + min_tokens, len(tokens))\n",
    "            chunk_tokens = tokens[i:end]\n",
    "        chunks.append(TOKENIZER_FOR_CHUNK.decode(chunk_tokens))\n",
    "        if end == len(tokens): break\n",
    "        i = max(end - overlap, 0)\n",
    "    return chunks\n",
    "\n",
    "# --- Models ---\n",
    "logger.info(\"Loading models...\")\n",
    "embedder = SentenceTransformer(CONFIG[\"EMBED_MODEL\"])\n",
    "reranker = CrossEncoder(CONFIG[\"RERANK_MODEL\"])\n",
    "gen_tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"GEN_MODEL\"])\n",
    "gen_model = AutoModelForSeq2SeqLM.from_pretrained(CONFIG[\"GEN_MODEL\"])\n",
    "\n",
    "# --- ChromaDB client ---\n",
    "client = chromadb.PersistentClient(path=CONFIG[\"PERSIST_DIR\"])\n",
    "if CONFIG[\"COLLECTION_NAME\"] in [c.name for c in client.list_collections()]:\n",
    "    client.delete_collection(CONFIG[\"COLLECTION_NAME\"])\n",
    "collection = client.create_collection(name=CONFIG[\"COLLECTION_NAME\"], metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "# --- Example docs (replace with your JD ingestion) ---\n",
    "docs = [\n",
    "    {\"source\":\"jd\",\"title\":\"Intern JD\",\"section\":\"Track B\",\"text\":\"Goal: Build and host a small RAG app...\"},\n",
    "    {\"source\":\"jd2\",\"title\":\"Intern JD\",\"section\":\"LLM\",\"text\":\"Use any provider... generate grounded answers with citations.\"},\n",
    "    {\"source\":\"jd3\",\"title\":\"Intern JD\",\"section\":\"Frontend\",\"text\":\"Frontend: upload/paste area, query box, answers panel... Hosting: deploy on free host...\"},\n",
    "    {\"source\":\"jd4\",\"title\":\"Intern JD\",\"section\":\"Acceptance\",\"text\":\"Acceptance: working URL; query → retrieved chunks → reranked → LLM answer with citations visible...\"}\n",
    "]\n",
    "\n",
    "# --- Indexing ---\n",
    "ids, metadatas, documents = [], [], []\n",
    "for d_i, d in enumerate(docs):\n",
    "    for c_i, chunk in enumerate(chunk_text(d[\"text\"])):\n",
    "        ids.append(f\"{d['source']}::{d['section']}::{d_i}-{c_i}-{uuid.uuid4().hex[:8]}\")\n",
    "        metadatas.append({\"source\":d[\"source\"],\"title\":d[\"title\"],\"section\":d[\"section\"],\"position\":c_i})\n",
    "        documents.append(chunk)\n",
    "embeddings = embedder.encode(documents, convert_to_numpy=True, normalize_embeddings=True)\n",
    "collection.add(ids=ids, metadatas=metadatas, documents=documents, embeddings=embeddings.tolist())\n",
    "logger.info(f\"Indexed {len(documents)} chunks.\")\n",
    "\n",
    "# --- Retrieval (MMR) ---\n",
    "def mmr(query_vec, doc_vecs, k=CONFIG[\"TOP_K\"], lambda_div=CONFIG[\"MMR_LAMBDA\"]):\n",
    "    sim_to_query = doc_vecs @ query_vec\n",
    "    selected, candidates = [], list(range(len(doc_vecs)))\n",
    "    while len(selected) < min(k, len(doc_vecs)):\n",
    "        if not selected:\n",
    "            idx = int(np.argmax(sim_to_query)); selected.append(idx); candidates.remove(idx)\n",
    "        else:\n",
    "            sim_to_selected = np.max(doc_vecs[selected] @ doc_vecs.T, axis=0)\n",
    "            mmr_scores = lambda_div * sim_to_query - (1 - lambda_div) * sim_to_selected\n",
    "            mmr_scores[selected] = -np.inf\n",
    "            idx = int(np.argmax(mmr_scores))\n",
    "            if idx in candidates: selected.append(idx); candidates.remove(idx)\n",
    "            else: break\n",
    "    return selected\n",
    "\n",
    "def rerank(query, docs_list, top_n=CONFIG[\"FINAL_K\"]):\n",
    "    scores = reranker.predict([(query, d) for d in docs_list])\n",
    "    order = np.argsort(-scores)[:top_n]\n",
    "    return [(docs_list[i], float(scores[i])) for i in order]\n",
    "\n",
    "def generate_answer(query, contexts):\n",
    "    prompt_context = \"\".join([f\"[{i}] {c['text']}\\nSource: {c['meta']['source']} | {c['meta']['section']}\\n\\n\" for i,c in enumerate(contexts,1)])\n",
    "    prompt = f\"Answer the query using ONLY context. Cite sources inline as [1], [2].\\nQuery: {query}\\n\\nContext:\\n{prompt_context}\\nAnswer:\"\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output_ids = gen_model.generate(**inputs, do_sample=True, temperature=CONFIG[\"TEMPERATURE\"], max_new_tokens=CONFIG[\"MAX_NEW_TOKENS\"])\n",
    "    return gen_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def query_pipeline(user_query):\n",
    "    q_vec = embedder.encode([user_query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    res = collection.query(query_embeddings=[q_vec.tolist()], n_results=CONFIG[\"TOP_K\"], include=[\"documents\",\"metadatas\",\"embeddings\"])\n",
    "    docs_list, metas_list, emb_list = res[\"documents\"][0], res[\"metadatas\"][0], np.array(res[\"embeddings\"][0])\n",
    "    mmr_idxs = mmr(q_vec, emb_list)\n",
    "    mmr_docs, mmr_metas = [docs_list[i] for i in mmr_idxs], [metas_list[i] for i in mmr_idxs]\n",
    "    reranked = rerank(user_query, mmr_docs)\n",
    "    contexts = [{\"text\":doc_text,\"meta\":mmr_metas[mmr_docs.index(doc_text)],\"score\":score} for doc_text,score in reranked]\n",
    "    answer = generate_answer(user_query, contexts)\n",
    "    citations = [{\"label\":f\"[{i}]\",\"source\":c[\"meta\"][\"source\"],\"section\":c[\"meta\"][\"section\"],\"snippet\":c[\"text\"][:100]} for i,c in enumerate(contexts,1)]\n",
    "    return {\"query\":user_query,\"answer\":answer,\"citations\":citations}\n",
    "\n",
    "# --- Demo ---\n",
    "if __name__ == \"__main__\":\n",
    "    result = query_pipeline(\"What are acceptance criteria?\")\n",
    "    print(\"Answer:\", result[\"answer\"])\n",
    "    print(\"Citations:\", result[\"citations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBg0KFV3b56p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nCD_63s7kHr"
   },
   "source": [
    "#Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJePpQzYb59c",
    "outputId": "db10c049-0afa-402c-8f0d-68924b4b688c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Context\n",
      "Citations: [{'label': '[1]', 'source': 'jd3', 'section': 'Frontend', 'snippet': 'frontend : upload / paste area, query box, answers panel... hosting : deploy on free host...'}, {'label': '[2]', 'source': 'jd2', 'section': 'LLM', 'snippet': 'use any provider... generate grounded answers with citations.'}, {'label': '[3]', 'source': 'jd', 'section': 'Track B', 'snippet': 'goal : build and host a small rag app...'}, {'label': '[4]', 'source': 'jd4', 'section': 'Acceptance', 'snippet': 'acceptance : working url ; query → retrieved chunks → reranked → llm answer with citations visible..'}]\n"
     ]
    }
   ],
   "source": [
    "result2 = query_pipeline(\"What is required in the frontend?\")\n",
    "print(\"Answer:\", result2[\"answer\"])\n",
    "print(\"Citations:\", result2[\"citations\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTro0nmw7oPE"
   },
   "source": [
    "#Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uq9OXk4Wb6E8",
    "outputId": "0df754da-a36e-47b3-dcd8-7a38616a597f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Query\n",
      "Citations: [{'label': '[1]', 'source': 'jd2', 'section': 'LLM', 'snippet': 'use any provider... generate grounded answers with citations.'}, {'label': '[2]', 'source': 'jd', 'section': 'Track B', 'snippet': 'goal : build and host a small rag app...'}, {'label': '[3]', 'source': 'jd4', 'section': 'Acceptance', 'snippet': 'acceptance : working url ; query → retrieved chunks → reranked → llm answer with citations visible..'}, {'label': '[4]', 'source': 'jd3', 'section': 'Frontend', 'snippet': 'frontend : upload / paste area, query box, answers panel... hosting : deploy on free host...'}]\n"
     ]
    }
   ],
   "source": [
    "result3 = query_pipeline(\"Summarize the overall responsibilities of the intern role.\")\n",
    "print(\"Answer:\", result3[\"answer\"])\n",
    "print(\"Citations:\", result3[\"citations\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEW4yGPN7x9p"
   },
   "source": [
    "#Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hN5ZQ5eAt7RW",
    "outputId": "a4376056-e3c2-4742-b3ff-a4e4e0f028fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: RAG\n",
      "Citations: [{'label': '[1]', 'source': 'jd', 'section': 'Track B', 'snippet': 'goal : build and host a small rag app...'}, {'label': '[2]', 'source': 'jd2', 'section': 'LLM', 'snippet': 'use any provider... generate grounded answers with citations.'}, {'label': '[3]', 'source': 'jd4', 'section': 'Acceptance', 'snippet': 'acceptance : working url ; query → retrieved chunks → reranked → llm answer with citations visible..'}, {'label': '[4]', 'source': 'jd3', 'section': 'Frontend', 'snippet': 'frontend : upload / paste area, query box, answers panel... hosting : deploy on free host...'}]\n"
     ]
    }
   ],
   "source": [
    "result4 = query_pipeline(\"what is RAG.\")\n",
    "print(\"Answer:\", result4[\"answer\"])\n",
    "print(\"Citations:\", result4[\"citations\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jrej1Dght6e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOA89xFc1SHH"
   },
   "source": [
    "#Summary: This notebook demonstrates a lightweight, Colab‑friendly Retrieval‑Augmented Generation (RAG) pipeline using ChromaDB and Hugging Face models. It showcases document chunking, retrieval, reranking, and answer generation with citations, tailored for job description analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D53TMAbywF-F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ol5pt2E2V2s"
   },
   "source": [
    "#Note: This Mini RAG model is a simulated, Colab‑friendly version built with AI tools, and there remains scope for further improvement."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
